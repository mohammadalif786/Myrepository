import os
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import datasets, transforms, models
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
import matplotlib.pyplot as plt

# --- 1. Configuration and Hyperparameters ---
# CORRECTED: Updated the data directory to the correct Kaggle input path.
DATA_DIR = "/kaggle/input/multi-cancer/Multi Cancer/Multi Cancer/Cervical Cancer"
# CORRECTED: Path for the pre-trained MobileNetV2 model weights.
MODEL_PATH = "/kaggle/input/mobile/pytorch/default/1/mobilenet_v2-7ebf99e0.pth"

# Device configuration (use GPU if available, otherwise CPU)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {DEVICE}")

# Hyperparameters
NUM_EPOCHS = 10
BATCH_SIZE = 32
LEARNING_RATE = 0.001
# CORRECTED: Determined NUM_CLASSES dynamically from the dataset folder structure.
# This makes the code more robust if you add or remove classes later.
try:
    NUM_CLASSES = len(os.listdir(DATA_DIR))
    print(f"Found {NUM_CLASSES} classes.")
except FileNotFoundError:
    print(f"Error: The directory '{DATA_DIR}' was not found. Please ensure the path is correct.")
    NUM_CLASSES = 2 # Fallback value

# --- 2. Data Preprocessing and Loading with 70:15:15 Split ---
# Define data transformations
# No changes needed here, but it's good practice to have separate transforms
# for training (with augmentation) and validation/testing (without augmentation).
train_transforms = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

val_test_transforms = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

try:
    # Load the entire dataset
    full_dataset = datasets.ImageFolder(DATA_DIR)

    # Define split sizes
    total_size = len(full_dataset)
    train_size = int(0.70 * total_size)
    val_size = int(0.15 * total_size)
    test_size = total_size - train_size - val_size

    # Split the dataset
    train_subset, val_subset, test_subset = random_split(
        full_dataset, [train_size, val_size, test_size],
        generator=torch.Generator().manual_seed(42) # for reproducibility
    )

    # IMPORTANT: Create new dataset instances for each subset to apply the correct transforms
    class CustomDataset(torch.utils.data.Dataset):
        def __init__(self, subset, transform=None):
            self.subset = subset
            self.transform = transform
        def __getitem__(self, index):
            x, y = self.subset[index]
            if self.transform:
                x = self.transform(x)
            return x, y
        def __len__(self):
            return len(self.subset)

    train_dataset = CustomDataset(train_subset, transform=train_transforms)
    val_dataset = CustomDataset(val_subset, transform=val_test_transforms)
    test_dataset = CustomDataset(test_subset, transform=val_test_transforms)

    # Create dataloaders
    # CORRECTED: Changed num_workers to 2 for better compatibility with Kaggle environments.
    dataloaders = {
        'train': DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2),
        'val': DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2),
        'test': DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)
    }

    dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset), 'test': len(test_dataset)}
    class_names = full_dataset.classes

    print(f"Dataset sizes: {dataset_sizes}")
    print(f"Class names: {class_names}")

except FileNotFoundError:
    print(f"Error: The directory '{DATA_DIR}' was not found. Please check the path.")
    # Exit gracefully if the dataset is not found
    exit()
except Exception as e:
    print(f"An error occurred during data loading: {e}")
    exit()

# --- 3. Model Definition ---
# Load a pre-trained MobileNetV2 model structure (without weights)
model = models.mobilenet_v2(weights=None)

# CORRECTED: Load the pre-trained weights from the specified file path.
# This avoids downloading and uses your provided model file.
try:
    model.load_state_dict(torch.load(MODEL_PATH))
    print("Pre-trained weights loaded successfully.")
except FileNotFoundError:
    print(f"Error: Model weights not found at '{MODEL_PATH}'. Training from scratch.")
except Exception as e:
    print(f"An error occurred while loading model weights: {e}. Training from scratch.")

# Freeze all layers in the base model
for param in model.parameters():
    param.requires_grad = False

# Replace the classifier for our specific task
model.classifier = nn.Sequential(
    nn.Dropout(0.2),
    nn.Linear(model.last_channel, NUM_CLASSES)
)

# Move the model to the specified device
model = model.to(DEVICE)

# Define the loss function and optimizer
# Only the parameters of the new classifier will be updated.
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.classifier.parameters(), lr=LEARNING_RATE)

# --- 4. Training Function ---
def train_model(model, criterion, optimizer, dataloaders, num_epochs=10):
    since = time.time()
    best_acc = 0.0
    train_acc_history = []
    val_acc_history = []

    # Create a temporary directory for saving the best model weights
    os.makedirs("/kaggle/working/models", exist_ok=True)
    best_model_path = "/kaggle/working/models/best_model_weights.pth"


    for epoch in range(num_epochs):
        print(f'Epoch {epoch}/{num_epochs - 1}')
        print('-' * 10)

        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()
            else:
                model.eval()

            running_loss = 0.0
            running_corrects = 0

            for inputs, labels in dataloaders[phase]:
                inputs = inputs.to(DEVICE)
                labels = labels.to(DEVICE)

                optimizer.zero_grad()

                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)

                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / dataset_sizes[phase]
            epoch_acc = running_corrects.double() / dataset_sizes[phase]

            if phase == 'train':
                train_acc_history.append(epoch_acc.item())
            else:
                val_acc_history.append(epoch_acc.item())

            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')

            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                torch.save(model.state_dict(), best_model_path)
                print("Model saved!")

    time_elapsed = time.time() - since
    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')
    print(f'Best val Acc: {best_acc:.4f}')
    return time_elapsed, train_acc_history, val_acc_history, best_model_path


# --- 5. Evaluation Function ---
def evaluate_model(model, dataloader, class_names):
    model.eval()
    all_labels = []
    all_preds = []
    all_probs = []
    
    since = time.time()
    
    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(DEVICE)
            labels = labels.to(DEVICE)
            
            outputs = model(inputs)
            probs = nn.functional.softmax(outputs, dim=1)
            _, preds = torch.max(outputs, 1)
            
            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(preds.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())
            
    test_time = time.time() - since
    print(f"Testing complete in {test_time:.2f}s")
    
    # Calculate metrics
    accuracy = accuracy_score(all_labels, all_preds)
    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)
    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)
    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)
    
    try:
        if NUM_CLASSES == 2:
            all_probs_positive = np.array(all_probs)[:, 1]
            auc = roc_auc_score(all_labels, all_probs_positive)
        else:
            # For multi-class, use one-vs-rest
            auc = roc_auc_score(all_labels, all_probs, multi_class='ovr', average='weighted')
    except ValueError:
        auc = 0.0
    
    cm = confusion_matrix(all_labels, all_preds)
    class_accuracy = cm.diagonal() / cm.sum(axis=1)
    
    print("\n--- Evaluation Metrics ---")
    print(f"Overall Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-Score: {f1:.4f}")
    print(f"AUC: {auc:.4f}")
    
    print("\nClass-wise Accuracy:")
    for i, acc in enumerate(class_accuracy):
        print(f"  - {class_names[i]}: {acc:.4f}")
        
    return test_time

# --- 6. Main Execution ---
if __name__ == '__main__':
    # Check if dataloaders were created successfully
    if 'dataloaders' in locals():
        print("Starting training...")
        train_start_time, train_acc_history, val_acc_history, best_model_path = train_model(model, criterion, optimizer, dataloaders, num_epochs=NUM_EPOCHS)
        
        print("\nStarting evaluation on the test set...")
        # Load the best model weights for evaluation
        model.load_state_dict(torch.load(best_model_path))
        test_time = evaluate_model(model, dataloaders['test'], class_names)
        
        print("\n--- Final Summary ---")
        print(f"Total Training Time: {train_start_time // 60:.0f}m {train_start_time % 60:.0f}s")
        print(f"Total Test Time: {test_time:.2f}s")

        # Plotting the training and validation accuracy
        plt.figure(figsize=(10, 6))
        plt.plot(range(1, NUM_EPOCHS + 1), train_acc_history, label='Training Accuracy')
        plt.plot(range(1, NUM_EPOCHS + 1), val_acc_history, label='Validation Accuracy')
        plt.title('Training and Validation Accuracy over Epochs')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.legend()
        plt.grid(True)
        plt.show()
    else:
        print("Execution halted due to data loading errors.")